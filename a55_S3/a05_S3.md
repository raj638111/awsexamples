
# Bucket

1. **Note**
   1. Is created on a specific region

# Objects

1. **Components**
   1. s3://bucket/somepath/file.txt
      1. `file.txt`: Is the **object name**
      2. `somepath`: **Prefix**
      3. `somepath/file.txt`: **Key**
2. **Max object size** 
   1. `5TB`
3. **Metadata**
   1. List of key / value pairs (System or user metadata)
4. **Tags**
   1. Can have upto `10` for security/lifecycle
5. **Verison ID**

# Access / Security

1. **Provide access to user**
   1. IAM policy attached to user to grant/deny API calls
2. **Provide access to EC2 instance**
   1. Create a role and attach it to EC2 instance that provides access to S3
3. **Bucket based policy**
   1. Is done from console and the policy is attached to the bucket
   2. Can be used to,
      1. Grant / Deny wide public access to bucket
      2. Make a cross account from a user from another account
   3. How to allow read access to bucket? 
      1. _bucket_ > Permissions > Disable `Block all public access`
      2. _bucket_ > Permissions > Bucket Policy > _Provide only read access_
         1. Note
            1. Add a `/*` to the arn to specify objects within the bucket
               1. Ex: `arn:aws:s3:::rj-demo/*`
6. **ACL Based access**
    1. Can be
        1. Object Level (or)
        2. Bucket level


# Static website hosting

1. Why am I getting 403 forbidden error?
   1. Ensure bucket policy allows public access
2. How to achieve this?
   1. Bucket > _dir that contains the static files_ > Properties > Static website hosting > Specify _index.html_ file for home page

# S3 versioning

1. Is enabled at bucket level
2. **Delete marker**
   1. Deletes are marked by `delete marker` (So that it is possible to retrieve an old version of the file)
   2. Deleting a delete marker will restore the previous version of the object
3. Deleting an explicit version deletes that version permanently (ie no delete market is created)
4. A new object which does not have any old version gets `null` version ID

# Replication

1. Replication is asynchronous b/w source & destination bucket
2. **Two types**
   1. CRR (Cross regional replication)
   2. SRR (Same regional replication)
3. **Steps needed to enable replication**
   1. Enable version in both soure & destination buckets
4. Source & destination buckets can be,
   1. in different region
   2. in different AWS account
5. **Use cases**
   1. CRR
      1. Compliance, low latency access, replication across accounts
   2. SRR
      1. log aggregation, replication b/w production & test accounts
6. Constraints
   1. Only new objects are replicated (after replication is enabled)
   2. Use `S3 batch replication` to replicate existing objects
   3. Delete Operations
      1. Delete markers can be replicated from source to target bucket (By default is `Disabled`)
      2. Deletions with version ID are not replicated (To avoid permanent / malicious deletes)
   4. No chaining of replication
      1. Ex
         1. If b1 has replication to b2 which has replication to b3
         2. Object created in b1 is not replicated to b3
7. How to set replication?
   1. Enable s3 versioning
   2. Source bucket > Management > Replication rules
   
         

# Storage classes

1. Available classes
   1. **Standard - General Purpose**
      1. Availability: 99.99%
      2. Low latency & high throughput
      3. Can sustain 2 concurrent facility failure
   2. **Standard - Infrequent Access (IA)**
      1. Use case
         1. When infrequently access, but requires rapid access
         2. Disaster recovery & backups
      2. Advantage: Lower cost that standard
      3. Availability: 99.9%
   3. One Zone-Infrequent Access
      1. Constraint
         1. High durability (99.99*) in single zone only. Data is lost if AZ is destroyed
         2. Availability: 99.5%
      2. Use case
         1. Store secondary copies of on prem data
   4. Glacier 
      1. Instant retrieval
         1. Millisecond retrieval: Great for data accessed once a quarter
         2. Minimum storage duration: 90 days
      2. Flexible retrieval 
         1. Types
            1. Expedited (1 to 5 minutes)
            2. Standard (3 to 5 hours)
            3. Bulk (5 to 12 hours)
               1. Is free retrieval cost (I believe??)
         2. Minimum storage: 90 days
      3. Deep archive
         1. Use case: For long term storage
         2. Two tiers
            1. Standard (12 hours)
            2. Bulk (48 hours)
         3. Minimum storage duration: 180 days
   5. Intelligent tiering
      1. Helps to move objects based on usage patterns
      2. Small monthly & auto tiering fee
      3. No retrieval charges
      4. Available types
         1. Frequent access tier (automatic, default)
         2. Infrequent access tier (automatic)
            1. Objects not accessed for 30 days
         3. Archive Instance access tier (automatic)
            1. Objects not access for 90 days
         4. Archive access tier (Optional)
            1. Configurable from 90 days to 700+ days
         5. Deep archive access tier Optional)
            1. Configurable from 180 days to 700+ days
2. Can move b/w classes
   1. Manually (or)
   2. S3 lifecycle policies


# Lifecycle rules

1. What can be set?
   1. Transition actions
   2. Expiration actions
   3. Prefix based rules
   4. Object tags based rules
2. How to decide when to transition?
   1. Use `S3 Analytics`

# Request pays

1. Bucket owner (Not requester) pays for storage & data transfer cost
2. With requester pay,
   1. Requester pays for the data transfer cost
   2. Note: Request needs to be authenticated in AWS


# Event notifications

1. Event can be sent to,
   1. SNS
   2. SQS
   3. Lambda function
2. Policy setup
   1. Attach resource access policy to SNS, SQS or Lambda, which allows S3 to send events
   2. Note: No IAM role is added to S3
3. Steps 
   1. Bucket > Properties > Event notification
4. **Event Notification with Amazon EventBridge**
   1. S3 > Event Bridge > Over 18 AWS destinations
   2. Provides advanced filtering with JSON rules
   3. Destination examples
      1. Step functions
      2. Kinesis / Firehose
   4. Additional features
      1. Archive, replay events & reliable delivery


# S3 Baseline performance

1. Scales to high rate requests, 100-200ms latency
2. Per second / per prefix in a bucket
   1. 3500: PUT/COPY/POST/DELETE
   2. 5500: GET/HEAD
3. Prefix example
   1. s3://bucket/folder1/sub1/file
      1. Here prefix is /folder/sub1
4. **Performance Optimization**
   1. `Multi-part upload`
      1. Recommended for files > 100MB
      2. Must use for files > 5GB
5. **S3 Transfer Acceleration**
   1. Increases the transfer speed by transferring the file to **AWS edge location** which then forwards the data to bucket in target region
   2. Available edge locations: 200 & growing
   3. Example: File in USA -public internet-> Edge location -Fast Private net-> S3 Bucket Australia
6. ***Byte range fetches*
   1. Is used to improve read performance (Example: downloads)
   2. Parallelize GETS by requesting specific byte ranges
   3. Use case
      1. Improve download sppeds
      2. Can be used to retrieve only partial file (Say for example to read header)
       

# Batch operations

Perform bulk operations at a sing request
1. Use cases
   1. Modify all object metadata & properties
   2. Copy objects
   3. Encrypt unencrypted objects
   4. Modify ACLs, tags
   5. Restore objects from S3 glacier
   6. Invoke lambda fn on each objects
2. Can also perform
   1. Retries
   2. Track progress
   3. Send completion notification
   4. Generate reports
3. Can be used with 
   1. `S3 Inventory` to get object list & `S3 Select` to filter the objects

# Storage Lens

Used to understand, analyze & optimize storage across entire AWS organizations

1. Can be used to
   1. Disover anomalies
   2. Identify cost efficiencies
   3. Apply data protection best practices
2. Can 
   1. aggregate data at
      1. Organization 
      2. Specific account
      3. Region
      4. Buckets or prefixes
   2. create default or custom dashboard
   3. be configured to export metrics to S3 bucket (CSV or Parquet format) 
3. Default dashboard
   1. Is preconfigured by AWS
   2. Cannot be deleted, but can be disabled
   3. Shows multi region & multi account data
4. Available metrics
   1. Summary metrics
      1. General insights
      2. Storage bytes, Object count
      3. Sample use case
         1. Identify fast growing bucket or prefixes
   2. Cost optimization metrics
      1. Non-current version storage bytes
      2. Incomplete multi-part upload storage bytes
      3. Use cases
         1. Identify incomplete m.part.upload older then 7 days
         2. Identify objects transitioned to lower-cost storage classes
   3. Data protection metrics
      1. Version enabled bucket count
      2. MFA delete enabled bucket count
      3. SSEKMS enabled bucket count
      4. Cross region replication rule count
      5. Use cases
         1. Identify buckets not following data protection best practices
   4. Access management metrics
      1. Provides insights on S3 object ownership
      2. Object ownership bucket owner enforced bucket count
      3. Use cases
         1. Identify object ownership setting our bucket use
   5. Event metrics
      1. Provide info on S3 event notifications
      2. EventNotificationEnabledBucketCount
   6. Performance metrics
      1. Provide info on S3 transfer acceleration
      2. TransferAccelerationEnabledBucketCount
   7. Activity Metrics
      1. Provide info about how storage is requested
      2. AllRequests, GetRequests, PutRequests, ListRequests, BytesDownloaded, ...
   8. Status code metrics
      1. 200 OKStatusCount, 403 ForbiddenErrorCount, 404 NotFoundErrorCount, ...
5. Free vs Paid metrics
   1. Free
      1. Contains 28 usage metrics
      2. Data available to query for 14 days
   2. Paid / advanced metrics
      1. Provides additional metrics around activity, cost optimization etc...
      2. Can be published to cloudwatch
      3. Provides metrics at prefix level
      4. Data available for query for 15 months
    
      